# Redis 配置文件示例
# Redis configuration file example.
# 启动使用配置文件
# ./redis-server /path/to/redis.conf

# 注意单位
# 1k => 1000 bytes
# 1kb => 1024 bytes
# 1m => 1000000 bytes
# 1mb => 1024*1024 bytes
# 1g => 1000000000 bytes
# 1gb => 1024*1024*1024 bytes
# 单位对于大小写是不敏感的 1GB 1Gb 1gB 是相同的

################ INCLUDES #################
# 可以引入一个或多个配置文件，引入文件也可以包含其他的文件
#
# 注意include 不会被命令 "CONFIG REWRITE" from admin 或者哨兵，你最好在启动时就引# 入，而避免在运行时重写配置
#
# 如果想让include 覆盖配置文件中的配置，你最好将include置于配置文件的最后一行
# include /path/to/local.conf
# include /path/to/other.conf

################# MODULES ##################
#
# 在启动时加载模块，如果加载失败，redis会启动失败；可用可能是因为使用了多个加载模块指令
#
# loadmodule /path/to/my_module.so
# loadmodule /path/to/other_module.so

################# NETWORK ##################
# 默认的如果没有指定bind配置，redis将允许所有网络接入
#
# 允许多个地址使用bind配置
#
# Examples:
#
# bind 192.168.1.100 10.0.0.1
# bind 127.0.0.1 ::1
#
bind 127.0.0.1


# 保护模式是一种安全措施，为了防止redis实例暴露在互联网中，被随意的访问
#
# 以下条件将触发保护模式：
#
# 1) 没有设置bind 配置
# 2) 没有配置密码
#
# redis server 将只接受IPV4，IPV6  127.0.0.1 and ::1 的连接
#
# 保护模式默认是开启的，如果你想使用其他地址访问，又不想配置密码，可以使用bind配置
protected-mode yes

# 连接监听端口，默认6379；设置为0将不监听
port 6379

# TCP 监听积压数
# 在高并发环境中，你需要设定更高的值来避免客户端连接缓慢的问题；
# 需要注意的是Linux内核默认配置也会影响该指标，所以你需要确保同时调高
# /proc/sys/net/core/somaxconn 与tcp_max_syn_backlogn 的值
tcp-backlog 511

# Unix socket.
# 指定Unix socket 请求连接地址，没有默认值；没有指定则不会监听
#
# unixsocket /tmp/redis.sock
# unixsocketperm 700

# 超时时间大于timeout，则关闭空闲连接；设置为0则不关闭
timeout 0

# TCP keepalive.
# 当值为非0时，用于发送ACKS用于保持通讯；主要有如下两个原因：
# 1） 发现掉线客户端
# 2） 保持与客户端的连接
#
# 在Linux环境下，用指定的值作为单位去发送ACKS（心跳）
# 注意关闭连接需要双倍的时间
# 在其他内核环境下，发送的频率依赖内核参数的设置
# 从3.2.1开始，默认值是300
tcp-keepalive 300

################# GENERAL ##################

# Redis默认不会后台运行，通过设置为yes可以使其后台运行
# 注意Redis后台运行时将会在/var/run/创建一个名为redis.pid的文件
daemonize no

# If you run Redis from upstart or systemd, Redis can interact with your
# supervision tree. Options:
#   supervised no      - no supervision interaction
#   supervised upstart - signal upstart by putting Redis into SIGSTOP mode
#   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET
#   supervised auto    - detect upstart or systemd method based on
#                        UPSTART_JOB or NOTIFY_SOCKET environment variables
# Note: these supervision methods only signal "process is ready."
#       They do not enable continuous liveness pings back to your supervisor.
supervised no


# 指定pid文件，Redis启动时创建，退出时移除
# 非后台运行状态不会创建，后台运行则会创建，没有指定默认是：/var/run/redis.pid
#
# 即使Redos不能成功创建pid文件，服务也会正常启动
pidfile /var/run/redis_6379.pid

# 指定日志级别
# 可以被指定为如下值：
# debug 用于开发、测试(a lot of information, useful for development/testing)
# verbose 详细
# notice 适用于生产环境，日志级别比较合适
# warning 只重要的，警告信息会被记录
loglevel notice

# 指定日志名称，也可以指定为空字符串
logfile ""

# 开启系统日志，只需设定syslog-enabled yes
#
# syslog-enabled no

# 设置系统日志id.
# syslog-ident redis

# 设置系统日志工具，必须为 USER 或者 LOCAL0-LOCAL7
# syslog-facility local0

# 设置数据库的数量，默认的数据库序列是0 ，可以通过 select<dbid>来切换
# dbid的取值范围是0 到 databases-1
databases 16

# 展示Redis LOGO
always-show-logo yes

################# SNAPSHOTTING ##################
#
# 数据持久化:
#   save <秒数><改变值数量>
#   save <seconds> <changes>
#
#   save 900 1 900秒内有1个key改变则保存
#   save 300 10 300秒内有10个key改变则保存
#   save 60 10000 60秒内有10000个key改变则保存
#
#   你也可以通过注释掉save，来使其失效
#
#   你也可以通过设置为空字符串来移除预先的配置，如下：
#   save ""

save 900 1
save 300 10
save 60 10000

# 当Redis进行写RDB快照失败后是否继续工作，yes：不继续工作；no：继续工作
stop-writes-on-bgsave-error yes

# 是否使用LZF压缩dump.rdb
# 默认使用yes会一直压缩，但是会有CPU消耗
# 设置为no，不会消耗CPU，但是需要更多的磁盘空间
rdbcompression yes

# Redis 5.0 版本开始，RDB文件末尾会防止一个CRC64 的校验数，一次来保证容错性，
#但是会有大约10%的性能损耗；如果你追求更高的性能，则可以关闭这项设置；它会创建一个
#0的检验数，以此来跳过检查
rdbchecksum yes

# rdb文件名称
dbfilename dump.rdb

# 工作目录
# RDB 跟 AOF 文件都会保存在该路径下
dir ./

############## REPLICATION ################

# 主从复制，使用slaveof 可以指定一个Redis实例作为另一个节点的从节点.
# 关于主从复制，有如下几点你需要知道：
# 1) Redis复制是异步的，但是你可以配置当一个主节点的slave节点数少于某个数值时，停止接收写入
# 2) 如果复制链接断开较短的时间，Redis从节点能够与主节点重新同步，你可以根据自己的需要设置积压值
# 3) 复制是自动的，不需要用户接入；当网络故障结束后，从节点会自动重连的朱接单，并且跟主节点保持同步
#
# slaveof <masterip> <masterport>

# 如果主节点需要密码验证，那么从节点访问主节点是需要进行密码验证
# masterauth <master-password>

# 从节点如果与主节点断开，它可能会有如下两种表现：
# 1) 如果slave-serve-stale-data 设置为yes (默认)，从节点仍会响应客户端请求，
#但是可能会响应过期数据
# 2) if slave-serve-stale-data 设置为no，从节点将会响应异常“SYNC with master #in progress”
#
slave-serve-stale-data yes

# 设置从节点只读
slave-read-only yes

# 两种复制策略: disk or socket.
#
# -------------------------------------------------------
# WARNING: SOCKET 复制是实验性的
# -------------------------------------------------------
# 新增从节点或者从节点重新连接不能执行继续复制，需要执行全量复制，有如下两种方式
#
# 1) Disk: 主节点会创建一个新的进程在硬盘上写RDB文件，然后将RDB文件传输给从节点
# 2) Socket: 主节点会创建一个新的进程通过socket传输给从节点，而不使用磁盘
repl-diskless-sync no

# socket 从节点复制延迟时间，避免设置为0
repl-diskless-sync-delay 5

# 主从心跳时间间隔，默认为10秒
# repl-ping-slave-period 10

# 设置复制超时时间将用于以下途径
#
# 1) 批量IO操作.
# 2) 主节点访问从节点超时.
# 3) 从节点访问主节点.
#
# 一定要确认复制超时时间要大于主从心跳时间，负责一个时间周期内主从节点只能传输少量数据
#
# repl-timeout 60
# 如果你配置为yes，Redis往从节点发送数据将使用较小的数据包和占用较少的带宽；但是它也可能使从节点出现延迟增加
#
# 如果你设置为no，数据延迟将会被减弱，相应的会占用较大的带宽；高并发环境中设置为yes也许是个不错的选择
repl-disable-tcp-nodelay no

# 设置复制积压大小；当从节点断开一段时间，积压是一个缓冲区，为了保证从节点重新连接时，不必进行全同步，而是同步断开后遗失的一部分
# 缓冲区大小越大，从节点断开时间可以越长；当至少有一个从节点连接时，缓冲区才会被分配。
#
# repl-backlog-size 1mb

# 当从节点与主节点超过一定时间后，缓冲区将会被释放；可以通过以下参数来设置时间；设置为0则表示，永远不释放。
#
# repl-backlog-ttl 3600

# 从节点有优先级是一个数字；用于哨兵节点选举从节点成为主节点；值越低，成为master的概率越大
# 设置为0，则表示该节点永远不会被选举成为主节点，默认值是100
#
slave-priority 100

# 当主节点的连接的从节点少于N时，并且从节点落后M秒时；主节点将不在接收写入操作；需要有N个从节点在线
#
# 如下示例要求，至少有3个从节点在线并且落后小于等于10秒
# min-slaves-to-write 3
# min-slaves-max-lag 10
#
# 默认min-slaves-to-write是0；表示不会禁用写
# min-slaves-max-lag is set to 10.

# Redis主节点可以列出连接的从节点的地址和端口；例如使用“INFO replication”命令；
# 可以通过以下选项来重写丛节点的ip以及端口
# slave-announce-ip 5.5.5.5
# slave-announce-port 1234

################ SECURITY ##################

# 要求客户端进行密码验证，"foobared"是密码，客户端通过auth foobared 可以进行验证
#
# requirepass foobared

# 命令重命名
#
# 可以利用这项配置，将危险的命令重命名，防止被人猜测
#
# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52
#
# 重命名成空字符串，可以禁用该命令
# rename-command CONFIG ""
#
# 请注意重命名命令被记录在AOF文件或者传输给从节点可能会导致一些问题

################ CLIENTS ################

# 最大客户端连接数，默认是10000；超过改数量，新的连接将会返回错误max number of clients reached
# maxclients 10000

################ CLIENTS ################

# 最大客户端连接数，默认是10000；超过改数量，新的连接将会返回错误max number of clients reached
# maxclients 10000

############### MEMORY MANAGEMENT ###############

# 设置内存使用限制字节数；当达到限制，redis将尝试maxmemory-policy来移除一些keys
# 如果Redis根据策略不可以移除keys，或者策略设置为noeviction；Redis将回复errors，
# 将停止像 like SET, LPUSH, and so on等操作，只支持GET 操作
# Redis通常是用LRU 或者 LFU cache 或者 noeviction
# 注意从节点的缓冲区不计算在maxmemory之内，为了避免主机内存消耗尽，maxmemory可以设置的小一些
# maxmemory <bytes>

# 内存策略，即Redis将如何选择淘汰，当达到maxmemory时；你可以设置一下几种：
# volatile-lru -> lru移除过期keys
# allkeys-lru -> lru移除任何keys
# volatile-lfu -> lfu移除过期keys
# allkeys-lfu -> lfu移除任何keys
# volatile-random -> 随机移除过期
# allkeys-random -> 随意移除任何
# volatile-ttl ->  移除即将过期
# noeviction -> 不移除
#
# LRU means Least Recently Used 最近使用
# LFU means Least Frequently Used 最常使用
#
# 经过以上任何策略都没有可移除的keys时，Redis将会在写操作时返回error
#
#       以下写操作: set setnx setex append
#       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd
#       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby
#       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby
#       getset mset msetnx exec sort
#
# 默认是以下淘汰策略：noeviction
#
# maxmemory-policy noeviction

# LRU，LFU，TTL 算法检测的样本数量
# maxmemory-samples 5

############################# LAZY FREEING ####################################

# Redis 有两种删除方式；其中一种是：DEL 它是同步阻塞的删除方式；这意味着删除时间复杂度较低的O(1),O(log_N)的数据时间消耗较小
# 但是删除一个数以百万计的集合数据时，Redis将阻塞非常长的时间。
# 基于以上原因，Redis也提供了非阻塞的删除方式：例如：UNLINK，ASYNC，FLUSHALL，FLUSHDB等命令用于异步的减少内存
#

# DEL，UNLINK，ASYNC，FLUSHALL，FLUSHDB 这些指令是用户控制的；然而Redis以下情节中又必须删除Keys或者清空整个库在：
# 1) 由于maxmemory或者maxmemory policy 进行淘汰
# 2) 数据过期
# 3) 数据已存在
# 4) 复制期间，从节点需要全量复制，将会清空原有从库数据
#
# 以上情节默认都是以阻塞的方式进行的，你也可以通过配置使其以非阻塞的方式进行
lazyfree-lazy-eviction no
lazyfree-lazy-expire no
lazyfree-lazy-server-del no
slave-lazy-flush no

############################## APPEND ONLY MODE ###############################

# 默认的Redis会保存数据在磁盘上，这种方式在需要应用中都是一种不错的方式，但是Redis进程出现问题
# 或者断电都将导致丢失几分钟的数据；AOF是一种替代的方式，它将提供更加优秀的持久化方式；使用AOF
# 默认的持久化策略将只丢失1秒的数据。
#
# AOF和RDB持久化可以同时被应用，但是出现问题，重新启动Redis时，AOF文件将首先被加载
# Please check http://redis.io/topics/persistence for more information.

appendonly no

# aof文件名

appendfilename "appendonly.aof"

# AOF支持以下几种模式
# no: 不主动刷盘，交由操作系统在合适时机处理；丢失数据不可控制
# always: 每次写操作之后刷盘；慢但是安全性高
# everysec: 每秒刷盘一次，比较适宜；默认
#
# More details please check the following article:
# http://antirez.com/post/redis-persistence-demystified.html
#
# appendfsync always
appendfsync everysec
# appendfsync no

# AOF重写或者BGSAVE会执行大量磁盘IO，当AOF刷盘策略设置为always或者everysec时，在一些Linux配置中fsync()调用将会阻塞Redis太久
# 设置为yes，意味着在BGSAVE或者BGREWRITEAOF期间不会对新的写入执行fsync()调用；这意味着有可能造成丢失30秒的数据
# 如果对延迟要求比较高，可以将改值设置为yes；否则还是设置为no，这将保证你的数据有更好的持久性

no-appendfsync-on-rewrite no

# Redis可以自动重写AOF文件，当AOF的大小增长到一定比例
# 工作原理：Redis会记录上次AOF文件重写后的大小，会拿文件当前大小跟基础大小比较，当文件大小达到指定比例，将触发重写
# 以下分别是设定重写比例以及基础文件大小
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# Redis启动阶段加载的AOF数据，但是AOF文件可能是不完整的；当Redis运行时突然宕机，或者挂载的ext4文件系统掉线都有可能造成AOF文件不完整
# aof-load-truncated设置为yes，Redis将尽可能多的加载数据，Redis可以继续运行；
# 设置为no，则必须要求用户使用“redis-check-aof”单元来修复AOF文件，否则无法正常运行。
aof-load-truncated yes

# 当重写AOF文件时，Redis可以使用RDB作为一个基准，用于快速重写和恢复；当开启时AOF文件将结合两种文件结构如下所示：
#   [RDB file][AOF tail]
# 当加载时识别出AOF文件已“REDIS”开头，会先加载RDB文件，然后继续加载AOF文件；
# 默认设置为no，优先加载AOF文件是默认配置
aof-use-rdb-preamble no

################################ LUA SCRIPTING  ###############################
# lua脚本最大执行时间，单位毫秒；设置为0，将无限制
lua-time-limit 5000

################################ REDIS CLUSTER  ###############################
#
# 开启集群，默认是不开启的
#
# cluster-enabled yes

# 配置文件名称；每个cluster节点有一个配置文件；此文件不需要手动配置，有Redis自动创建和更新；
# cluster-config-file nodes-6379.conf

# Cluster节点超过超时时间，将被称为failure 状态
# cluster-node-timeout 15000

# 如果从节点的数据过于陈旧，那么它应该避免被选举成功master。
# 有如下两种检查方式，确认其数据是否陈旧：
# 1) 如果有多个salve节点可以进行故障转移，Slave节点间将会相互通信并进行排名，排名高的会优先成为Master
# 2) 比较每个单节点的Slave节点跟Master节点的交互时间；上次交互时间最久远的Slave节点将不会被选举出来
# 第2点用户可以调整
# cluster-slave-validity-factor 10

# 集群转移界限；默认是1；设置为0只用于DEBUG，在生产使用是非常危险的；禁用转移只需将其设置为一个很大的数字
# cluster-migration-barrier 1

# 默认如果集群不能覆盖全部插槽，集群将停止接收请求；如果你想让其继续工作可以设置为no
# cluster-require-full-coverage yes

# 设置为yes，可以防止Slaves进行故障转移，当然master还是可以手动故障转移；
# cluster-slave-no-failover no


########################## CLUSTER DOCKER/NAT support  ########################

# 集群声明ip，端口，bus-port
#
# cluster-announce-ip 10.1.1.5
# cluster-announce-port 6379
# cluster-announce-bus-port 6380

################################## SLOW LOG ###################################

# 用于记录慢请求日志，单位是微妙，所以1000000 是1秒；设置为0将每次都记录
slowlog-log-slower-than 10000

# 长度没有限制，只有有足够的内存就可以；可以通过SLOWLOG RESET 来释放内存
slowlog-max-len 128

################################ LATENCY MONITOR ##############################


# 延迟监控使用于监控Redis执行比较慢的操作，只会监控大于设定值的操作；0位不监控；单位：毫秒
# 如果你有延迟问题，你可以通过"CONFIG SET latency-monitor-threshold <milliseconds>" 来动态设置
latency-monitor-threshold 0

############################# EVENT NOTIFICATION ##############################

# http://redis.io/topics/notifications
# Redis可以用于事件的订阅/发布
# For instance if keyspace events notification is enabled, and a client
# performs a DEL operation on key "foo" stored in the Database 0, two
# messages will be published via Pub/Sub:
#
# PUBLISH __keyspace@0__:foo del
# PUBLISH __keyevent@0__:del foo
#
# It is possible to select the events that Redis will notify among a set
# of classes. Every class is identified by a single character:
#
#  K     Keyspace events, published with __keyspace@<db>__ prefix.
#  E     Keyevent events, published with __keyevent@<db>__ prefix.
#  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...
#  $     String commands
#  l     List commands
#  s     Set commands
#  h     Hash commands
#  z     Sorted set commands
#  x     Expired events (events generated every time a key expires)
#  e     Evicted events (events generated when a key is evicted for maxmemory)
#  A     Alias for g$lshzxe, so that the "AKE" string means all the events.
#
notify-keyspace-events ""

############################### ADVANCED CONFIG ###############################

# Hashes are encoded using a memory efficient data structure when they have a
# small number of entries, and the biggest entry does not exceed a given
# threshold. These thresholds can be configured using the following directives.
hash-max-ziplist-entries 512
hash-max-ziplist-value 64

# Lists are also encoded in a special way to save a lot of space.
# The number of entries allowed per internal list node can be specified
# as a fixed maximum size or a maximum number of elements.
# For a fixed maximum size, use -5 through -1, meaning:
# -5: max size: 64 Kb  <-- not recommended for normal workloads
# -4: max size: 32 Kb  <-- not recommended
# -3: max size: 16 Kb  <-- probably not recommended
# -2: max size: 8 Kb   <-- good
# -1: max size: 4 Kb   <-- good
# Positive numbers mean store up to _exactly_ that number of elements
# per list node.
# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),
# but if your use case is unique, adjust the settings as necessary.
list-max-ziplist-size -2

# Lists may also be compressed.
# Compress depth is the number of quicklist ziplist nodes from *each* side of
# the list to *exclude* from compression.  The head and tail of the list
# are always uncompressed for fast push/pop operations.  Settings are:
# 0: disable all list compression
# 1: depth 1 means "don't start compressing until after 1 node into the list,
#    going from either the head or tail"
#    So: [head]->node->node->...->node->[tail]
#    [head], [tail] will always be uncompressed; inner nodes will compress.
# 2: [head]->[next]->node->node->...->node->[prev]->[tail]
#    2 here means: don't compress head or head->next or tail->prev or tail,
#    but compress all nodes between them.
# 3: [head]->[next]->[next]->node->node->...->node->[prev]->[prev]->[tail]
# etc.
list-compress-depth 0

# Sets have a special encoding in just one case: when a set is composed
# of just strings that happen to be integers in radix 10 in the range
# of 64 bit signed integers.
# The following configuration setting sets the limit in the size of the
# set in order to use this special memory saving encoding.
set-max-intset-entries 512

# Similarly to hashes and lists, sorted sets are also specially encoded in
# order to save a lot of space. This encoding is only used when the length and
# elements of a sorted set are below the following limits:
zset-max-ziplist-entries 128
zset-max-ziplist-value 64

# HyperLogLog sparse representation bytes limit. The limit includes the
# 16 bytes header. When an HyperLogLog using the sparse representation crosses
# this limit, it is converted into the dense representation.
#
# A value greater than 16000 is totally useless, since at that point the
# dense representation is more memory efficient.
#
# The suggested value is ~ 3000 in order to have the benefits of
# the space efficient encoding without slowing down too much PFADD,
# which is O(N) with the sparse encoding. The value can be raised to
# ~ 10000 when CPU is not a concern, but space is, and the data set is
# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.
hll-sparse-max-bytes 3000

# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in
# order to help rehashing the main Redis hash table (the one mapping top-level
# keys to values). The hash table implementation Redis uses (see dict.c)
# performs a lazy rehashing: the more operation you run into a hash table
# that is rehashing, the more rehashing "steps" are performed, so if the
# server is idle the rehashing is never complete and some more memory is used
# by the hash table.
#
# The default is to use this millisecond 10 times every second in order to
# actively rehash the main dictionaries, freeing memory when possible.
#
# If unsure:
# use "activerehashing no" if you have hard latency requirements and it is
# not a good thing in your environment that Redis can reply from time to time
# to queries with 2 milliseconds delay.
#
# use "activerehashing yes" if you don't have such hard requirements but
# want to free memory asap when possible.
activerehashing yes

# The client output buffer limits can be used to force disconnection of clients
# that are not reading data from the server fast enough for some reason (a
# common reason is that a Pub/Sub client can't consume messages as fast as the
# publisher can produce them).
#
# The limit can be set differently for the three different classes of clients:
#
# normal -> normal clients including MONITOR clients
# slave  -> slave clients
# pubsub -> clients subscribed to at least one pubsub channel or pattern
#
# The syntax of every client-output-buffer-limit directive is the following:
#
# client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds>
#
# A client is immediately disconnected once the hard limit is reached, or if
# the soft limit is reached and remains reached for the specified number of
# seconds (continuously).
# So for instance if the hard limit is 32 megabytes and the soft limit is
# 16 megabytes / 10 seconds, the client will get disconnected immediately
# if the size of the output buffers reach 32 megabytes, but will also get
# disconnected if the client reaches 16 megabytes and continuously overcomes
# the limit for 10 seconds.
#
# By default normal clients are not limited because they don't receive data
# without asking (in a push way), but just after a request, so only
# asynchronous clients may create a scenario where data is requested faster
# than it can read.
#
# Instead there is a default limit for pubsub and slave clients, since
# subscribers and slaves receive data in a push fashion.
#
# Both the hard or the soft limit can be disabled by setting them to zero.
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit slave 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60

# Client query buffers accumulate new commands. They are limited to a fixed
# amount by default in order to avoid that a protocol desynchronization (for
# instance due to a bug in the client) will lead to unbound memory usage in
# the query buffer. However you can configure it here if you have very special
# needs, such us huge multi/exec requests or alike.
#
# client-query-buffer-limit 1gb

# In the Redis protocol, bulk requests, that are, elements representing single
# strings, are normally limited ot 512 mb. However you can change this limit
# here.
#
# proto-max-bulk-len 512mb

# Redis calls an internal function to perform many background tasks, like
# closing connections of clients in timeout, purging expired keys that are
# never requested, and so forth.
#
# Not all tasks are performed with the same frequency, but Redis checks for
# tasks to perform according to the specified "hz" value.
#
# By default "hz" is set to 10. Raising the value will use more CPU when
# Redis is idle, but at the same time will make Redis more responsive when
# there are many keys expiring at the same time, and timeouts may be
# handled with more precision.
#
# The range is between 1 and 500, however a value over 100 is usually not
# a good idea. Most users should use the default of 10 and raise this up to
# 100 only in environments where very low latency is required.
hz 10

# When a child rewrites the AOF file, if the following option is enabled
# the file will be fsync-ed every 32 MB of data generated. This is useful
# in order to commit the file to the disk more incrementally and avoid
# big latency spikes.
aof-rewrite-incremental-fsync yes

# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good
# idea to start with the default settings and only change them after investigating
# how to improve the performances and how the keys LFU change over time, which
# is possible to inspect via the OBJECT FREQ command.
#
# There are two tunable parameters in the Redis LFU implementation: the
# counter logarithm factor and the counter decay time. It is important to
# understand what the two parameters mean before changing them.
#
# The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis
# uses a probabilistic increment with logarithmic behavior. Given the value
# of the old counter, when a key is accessed, the counter is incremented in
# this way:
#
# 1. A random number R between 0 and 1 is extracted.
# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).
# 3. The counter is incremented only if R < P.
#
# The default lfu-log-factor is 10. This is a table of how the frequency
# counter changes with a different number of accesses with different
# logarithmic factors:
#
# +--------+------------+------------+------------+------------+------------+
# | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |
# +--------+------------+------------+------------+------------+------------+
# | 0      | 104        | 255        | 255        | 255        | 255        |
# +--------+------------+------------+------------+------------+------------+
# | 1      | 18         | 49         | 255        | 255        | 255        |
# +--------+------------+------------+------------+------------+------------+
# | 10     | 10         | 18         | 142        | 255        | 255        |
# +--------+------------+------------+------------+------------+------------+
# | 100    | 8          | 11         | 49         | 143        | 255        |
# +--------+------------+------------+------------+------------+------------+
#
# NOTE: The above table was obtained by running the following commands:
#
#   redis-benchmark -n 1000000 incr foo
#   redis-cli object freq foo
#
# NOTE 2: The counter initial value is 5 in order to give new objects a chance
# to accumulate hits.
#
# The counter decay time is the time, in minutes, that must elapse in order
# for the key counter to be divided by two (or decremented if it has a value
# less <= 10).
#
# The default value for the lfu-decay-time is 1. A Special value of 0 means to
# decay the counter every time it happens to be scanned.
#
# lfu-log-factor 10
# lfu-decay-time 1

########################### ACTIVE DEFRAGMENTATION #######################
#
# WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested
# even in production and manually tested by multiple engineers for some
# time.
#
# What is active defragmentation?
# -------------------------------
#
# Active (online) defragmentation allows a Redis server to compact the
# spaces left between small allocations and deallocations of data in memory,
# thus allowing to reclaim back memory.
#
# Fragmentation is a natural process that happens with every allocator (but
# less so with Jemalloc, fortunately) and certain workloads. Normally a server
# restart is needed in order to lower the fragmentation, or at least to flush
# away all the data and create it again. However thanks to this feature
# implemented by Oran Agra for Redis 4.0 this process can happen at runtime
# in an "hot" way, while the server is running.
#
# Basically when the fragmentation is over a certain level (see the
# configuration options below) Redis will start to create new copies of the
# values in contiguous memory regions by exploiting certain specific Jemalloc
# features (in order to understand if an allocation is causing fragmentation
# and to allocate it in a better place), and at the same time, will release the
# old copies of the data. This process, repeated incrementally for all the keys
# will cause the fragmentation to drop back to normal values.
#
# Important things to understand:
#
# 1. This feature is disabled by default, and only works if you compiled Redis
#    to use the copy of Jemalloc we ship with the source code of Redis.
#    This is the default with Linux builds.
#
# 2. You never need to enable this feature if you don't have fragmentation
#    issues.
#
# 3. Once you experience fragmentation, you can enable this feature when
#    needed with the command "CONFIG SET activedefrag yes".
#
# The configuration parameters are able to fine tune the behavior of the
# defragmentation process. If you are not sure about what they mean it is
# a good idea to leave the defaults untouched.

# Enabled active defragmentation
# activedefrag yes

# Minimum amount of fragmentation waste to start active defrag
# active-defrag-ignore-bytes 100mb

# Minimum percentage of fragmentation to start active defrag
# active-defrag-threshold-lower 10

# Maximum percentage of fragmentation at which we use maximum effort
# active-defrag-threshold-upper 100

# Minimal effort for defrag in CPU percentage
# active-defrag-cycle-min 25

# Maximal effort for defrag in CPU percentage
# active-defrag-cycle-max 75
